{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":174871011,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNet-LSTM-400","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os as os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nsns.set_style(\"darkgrid\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim.lr_scheduler as lr_scheduler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T08:48:57.320768Z","iopub.execute_input":"2024-05-17T08:48:57.321745Z","iopub.status.idle":"2024-05-17T08:49:04.017406Z","shell.execute_reply.started":"2024-05-17T08:48:57.321709Z","shell.execute_reply":"2024-05-17T08:49:04.016172Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:04.019243Z","iopub.execute_input":"2024-05-17T08:49:04.019738Z","iopub.status.idle":"2024-05-17T08:49:04.026618Z","shell.execute_reply.started":"2024-05-17T08:49:04.019709Z","shell.execute_reply":"2024-05-17T08:49:04.025128Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"seed = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:04.028134Z","iopub.execute_input":"2024-05-17T08:49:04.029005Z","iopub.status.idle":"2024-05-17T08:49:04.040930Z","shell.execute_reply.started":"2024-05-17T08:49:04.028964Z","shell.execute_reply":"2024-05-17T08:49:04.039588Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# ResNet-LSTM architecture","metadata":{}},{"cell_type":"code","source":"# CNN ###############################\nclass ResNet_block(nn.Module):    \n    def __init__(self, filters, kernels):\n        super().__init__()\n        f1, f2, f3 = filters\n        k1, k2, k3 = kernels\n        self.conv1 = nn.Conv2d(1,f1, k1, 1)\n        self.bn1 = nn.BatchNorm2d(f1)\n        self.conv2 = nn.Conv2d(f1,f2, k2, 1)\n        self.bn2 = nn.BatchNorm2d(f2)\n        self.conv3 = nn.Conv2d(f2,f3, k3, 1)\n        self.maxpool = nn.MaxPool2d(2,2)\n        self.flat = nn.Flatten()\n        self.relu = nn.ReLU()\n        #this layer is to get the identity to have\n        #the same shape as the other branch, bias = False\n        self.fc0 = nn.Linear(320,256, bias = False)\n        #convolution branch/main branch\n        self.conv_total = nn.Sequential(\n            self.conv1,\n            self.maxpool,\n            #self.bn1,\n            self.relu,\n            self.conv2,\n            self.maxpool,\n            #self.bn2,\n            self.relu,\n            self.conv3,\n            self.flat\n        )\n        #shortcut branch\n        self.shortcut = nn.Sequential(self.flat, self.fc0)\n\n    def forward(self,x):\n        #x_i and x must have the same shape\n        x_i = self.shortcut(x)\n        x = self.conv_total(x)\n        xt = x+ x_i\n        #xt = torch.cat([x, x_i], dim = 1) \n        return xt","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:04.042869Z","iopub.execute_input":"2024-05-17T08:49:04.043205Z","iopub.status.idle":"2024-05-17T08:49:04.054633Z","shell.execute_reply.started":"2024-05-17T08:49:04.043178Z","shell.execute_reply":"2024-05-17T08:49:04.053539Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class RNN_block(nn.Module):\n    def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):\n        super(RNN_block,self).__init__()\n        self.D = n_inputs\n        self.M = n_hidden\n        self.K = n_outputs\n        self.L = n_rnnlayers        \n        self.lstm = nn.LSTM(input_size = self.D,\n                           hidden_size = self.M,\n                           num_layers = self.L,\n                           batch_first = True)    \n    def forward(self, X):\n        #input X is NxTxD\n        #initial hidden state h\n        #for bidirectional: Need to multiply L by 2\n        #h0 = torch.zeros(self.L*2, X.size(0), self.M).to(device)\n        #non-bidirectional\n        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n        #output is NxTx2M for birdir, NxTxM for non-bidir\n        out, _ = self.lstm(X, (h0, c0))\n        #we only want h(T) at the final time step\n        # output is now of shape (N, M)\n        out = out[:, -1, :]\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:08.647166Z","iopub.execute_input":"2024-05-17T08:49:08.648118Z","iopub.status.idle":"2024-05-17T08:49:08.656784Z","shell.execute_reply.started":"2024-05-17T08:49:08.648083Z","shell.execute_reply":"2024-05-17T08:49:08.655854Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ResNet_RNN_hybrid(nn.Module):\n    def __init__(self, resnet_block, rnn_block, feat_vec_size):\n        super(ResNet_RNN_hybrid, self).__init__()\n        self.resnet_block = resnet_block\n        self.rnn_block = rnn_block\n        self.feat_vec_size = feat_vec_size\n        self.fc1 = nn.Linear(self.feat_vec_size, 1024)\n        self.fc2 = nn.Linear(1024, 4)\n        \n    def forward(self, x):\n        #output of cnn block is (N,848)\n        x1 = x.view(-1,1, 16,20)\n        x1 = self.resnet_block(x1)\n        #output of rnn block is (N,M )\n        x2 = self.rnn_block(x)\n        #concatenate the 2 outputs to produce a feat vec (N, M+208)\n        xx = torch.cat([x1, x2], dim = 1)\n        # pass through linear layers\n        xx = self.fc1(xx)\n        #final output is 4\n        xx = self.fc2(xx)      \n        return xx","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:09.991429Z","iopub.execute_input":"2024-05-17T08:49:09.991860Z","iopub.status.idle":"2024-05-17T08:49:10.001571Z","shell.execute_reply.started":"2024-05-17T08:49:09.991824Z","shell.execute_reply":"2024-05-17T08:49:10.000098Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"resnet_block = ResNet_block([128,64,128], [4,3,2])\nresnet_block.to(device)\n\nlstm_block = RNN_block(20, 400, 2, 4)\nlstm_block.to(device)\n\nmodel = ResNet_RNN_hybrid(resnet_block, lstm_block, 400+256)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:11.473055Z","iopub.execute_input":"2024-05-17T08:49:11.473448Z","iopub.status.idle":"2024-05-17T08:49:11.583163Z","shell.execute_reply.started":"2024-05-17T08:49:11.473419Z","shell.execute_reply":"2024-05-17T08:49:11.581951Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ResNet_RNN_hybrid(\n  (resnet_block): ResNet_block(\n    (conv1): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (flat): Flatten(start_dim=1, end_dim=-1)\n    (relu): ReLU()\n    (fc0): Linear(in_features=320, out_features=256, bias=False)\n    (conv_total): Sequential(\n      (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (2): ReLU()\n      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): ReLU()\n      (6): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n      (7): Flatten(start_dim=1, end_dim=-1)\n    )\n    (shortcut): Sequential(\n      (0): Flatten(start_dim=1, end_dim=-1)\n      (1): Linear(in_features=320, out_features=256, bias=False)\n    )\n  )\n  (rnn_block): RNN_block(\n    (lstm): LSTM(20, 400, num_layers=2, batch_first=True)\n  )\n  (fc1): Linear(in_features=656, out_features=1024, bias=True)\n  (fc2): Linear(in_features=1024, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"#count the number of parameters in the model\nparams_resnet = [p.numel() for p in resnet_block.parameters() if p.requires_grad]\nprint(f'Resnet parameters: {sum(params_resnet)}')\n\n#count the number of parameters in the model\nparams_lstm = [p.numel() for p in lstm_block.parameters() if p.requires_grad]\nprint(f'LSTM block parameters: {sum(params_lstm)}')\n\n\n#count the number of parameters in the model\nparams = [p.numel() for p in model.parameters() if p.requires_grad]\nprint(f'Total parameters: {sum(params)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:49:56.962184Z","iopub.execute_input":"2024-05-17T08:49:56.962608Z","iopub.status.idle":"2024-05-17T08:49:56.970836Z","shell.execute_reply.started":"2024-05-17T08:49:56.962573Z","shell.execute_reply":"2024-05-17T08:49:56.969727Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Resnet parameters: 191168\nLSTM block parameters: 1958400\nTotal parameters: 2826436\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load data, define dataset and dataloaders","metadata":{}},{"cell_type":"code","source":"X_train = np.load('/kaggle/input/cicy4-data-processing/conf_Xtrain.npy')\nX_test = np.load('/kaggle/input/cicy4-data-processing/conf_Xtest.npy')\nX_valid = np.load('/kaggle/input/cicy4-data-processing/conf_Xvalid.npy')\n\ny_train = np.load('/kaggle/input/cicy4-data-processing/hodge_ytrain.npy')\ny_test= np.load('/kaggle/input/cicy4-data-processing/hodge_ytest.npy')\ny_valid = np.load('/kaggle/input/cicy4-data-processing/hodge_yvalid.npy')\n\nprint(X_train.shape, X_valid.shape, X_test.shape)\nprint(y_train.shape, y_valid.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:15.076645Z","iopub.execute_input":"2024-05-17T08:50:15.077047Z","iopub.status.idle":"2024-05-17T08:50:30.397113Z","shell.execute_reply.started":"2024-05-17T08:50:15.077014Z","shell.execute_reply":"2024-05-17T08:50:30.395940Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(652092, 16, 20) (72455, 16, 20) (181137, 16, 20)\n(652092, 4) (72455, 4) (181137, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Convert data to torch tensor with float32 precision \n#(needed to be compatible with the floating decision of the network parameters)\nX_train = torch.from_numpy(X_train.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\n\nX_valid = torch.from_numpy(X_valid.astype(np.float32)) \ny_valid = torch.from_numpy(y_valid.astype(np.float32)) \n\nX_test = torch.from_numpy(X_test.astype(np.float32)) \ny_test= torch.from_numpy(y_test.astype(np.float32))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:30.399239Z","iopub.execute_input":"2024-05-17T08:50:30.399573Z","iopub.status.idle":"2024-05-17T08:50:30.985784Z","shell.execute_reply.started":"2024-05-17T08:50:30.399543Z","shell.execute_reply":"2024-05-17T08:50:30.984679Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CICY4Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y        \n    def __len__(self):\n        return len(self.X)      \n    def __getitem__(self, idx):\n        X0 = self.X[idx]\n        y0 = self.y[idx]  \n        return X0, y0\n        \ntrain_set = CICY4Dataset(X_train, y_train)\nval_set = CICY4Dataset(X_valid, y_valid)\ntest_set = CICY4Dataset(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:30.987124Z","iopub.execute_input":"2024-05-17T08:50:30.987458Z","iopub.status.idle":"2024-05-17T08:50:30.996385Z","shell.execute_reply.started":"2024-05-17T08:50:30.987430Z","shell.execute_reply":"2024-05-17T08:50:30.995017Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:30.999220Z","iopub.execute_input":"2024-05-17T08:50:30.999652Z","iopub.status.idle":"2024-05-17T08:50:31.007721Z","shell.execute_reply.started":"2024-05-17T08:50:30.999608Z","shell.execute_reply":"2024-05-17T08:50:31.006459Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(f'Train loader contains {len(train_loader)} batches, each of size 128')\nprint(f'Valid loader contains {len(val_loader)} batches, each of size 128')\nprint(f'Test loader contains {len(test_loader)} batches, each of size 128')\n#d, t = next(iter(train_loader))\n#print(d.dtype)\n#print('Each data and target batch have the following shape')\n#print(d.shape, t.shape)\n\nfor d, t in test_loader:\n    print(d.dtype)\n    print('Each data and target batch have the following shape')\n    print(d.shape, t.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:31.009257Z","iopub.execute_input":"2024-05-17T08:50:31.009626Z","iopub.status.idle":"2024-05-17T08:50:31.049396Z","shell.execute_reply.started":"2024-05-17T08:50:31.009598Z","shell.execute_reply":"2024-05-17T08:50:31.048158Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Train loader contains 5095 batches, each of size 128\nValid loader contains 567 batches, each of size 128\nTest loader contains 1416 batches, each of size 128\ntorch.float32\nEach data and target batch have the following shape\ntorch.Size([128, 16, 20]) torch.Size([128, 4])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utility functions for training","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, criterion, optimizer, train_loader, cnn = False):\n    model.train()\n    train_loss = []\n    for inputs, target in train_loader:\n        if cnn:\n            inputs = inputs.view(batch_size,1,16,20)\n        inputs, target = inputs.to(device), target.to(device)\n        optimizer.zero_grad()\n        out = model(inputs)\n        loss = criterion(out, target)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n    #average the train_loss list in for all batches in the train_gen\n    train_loss = np.mean(train_loss)    \n    return train_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:50.894622Z","iopub.execute_input":"2024-05-17T08:50:50.895042Z","iopub.status.idle":"2024-05-17T08:50:50.903057Z","shell.execute_reply.started":"2024-05-17T08:50:50.895008Z","shell.execute_reply":"2024-05-17T08:50:50.902002Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def validate_one_epoch(model, criterion, optimizer, val_loader, cnn = False):\n    model.eval()\n    test_loss = []\n    for inputs, target in val_loader:\n        if cnn:\n            inputs = inputs.view(batch_size,1,16,20)\n        inputs, target = inputs.to(device), target.to(device)\n        out = model(inputs)\n        loss = criterion(out, target)\n        test_loss.append(loss.item())\n    #average the test_loss list in for all batches in the test_gen\n    test_loss = np.mean(test_loss)\n    return test_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:50:52.769443Z","iopub.execute_input":"2024-05-17T08:50:52.769823Z","iopub.status.idle":"2024-05-17T08:50:52.777594Z","shell.execute_reply.started":"2024-05-17T08:50:52.769793Z","shell.execute_reply":"2024-05-17T08:50:52.776353Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def batch_gd_scheduler(model, new_model_name, criterion, optimizer, train_loader, val_loader, scheduler,\n                         epochs, device, batch_size=128,  cnn = False):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    patience = 0\n    best_loss = 1000\n    max_patience = 20\n    for i in range(epochs):\n        t0 = datetime.now()\n        train_loss = train_one_epoch(model, criterion, optimizer,train_loader)\n        test_loss = validate_one_epoch(model, criterion, optimizer,val_loader)\n        #Early stopping based on test loss\n        if i == 0:\n            best_loss = test_loss   \n            torch.save(model, f'/kaggle/working/{new_model_name}.pt')\n            print(f'Model saved as {new_model_name} at epoch {i}')\n        else:\n            if test_loss < best_loss:\n                best_loss = test_loss\n                torch.save(model, f'/kaggle/working/{new_model_name}.pt')\n                print(f'Model overwritten at epoch {i}, new best val loss {best_loss: .4f}')\n                patience = 0\n            else:\n                patience = patience +1\n                #print(f'No improvement, current patience level is {patience} at epoch {i}')\n        if patience > max_patience:\n            print(f'Max patience reached, training is terminated at epoch {i}')\n            break\n        #if early_stopper.early_stop(test_loss):             \n        #    break\n        #Apply scheduler after the train+validate parts\n        before_lr = optimizer.param_groups[0][\"lr\"]\n        scheduler.step(test_loss)\n        after_lr = optimizer.param_groups[0][\"lr\"]\n        train_losses[i] = train_loss\n        test_losses[i] = test_loss\n        #write the losses into a csv file\n        loss_dict = {'train_loss': train_losses, 'test_loss': test_losses}\n        dd = pd.DataFrame(loss_dict)\n        dd.to_csv('loss_dict_resnet_lstm_400.csv', index = False)\n        dt = datetime.now()-t0\n        if i%10==0:\n            print(f'Epoch: {i}/{epochs}, train loss: {train_loss: .3f}, val_loss: {test_loss: .3f}, duration: {dt}, learning rate: {before_lr, after_lr}')\n    return train_losses, test_losses","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:03.824878Z","iopub.execute_input":"2024-05-17T08:51:03.825291Z","iopub.status.idle":"2024-05-17T08:51:03.838787Z","shell.execute_reply.started":"2024-05-17T08:51:03.825259Z","shell.execute_reply":"2024-05-17T08:51:03.837542Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train_from_scratch_or_load(load_model_weight,new_model_name, epochs, cnn = False):    \n    if load_model_weight==None:\n        #criterion = nn.MSELoss()\n        criterion = nn.HuberLoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, min_lr = 1e-7)\n        train_losses, test_losses=batch_gd_scheduler(model, new_model_name, criterion, \n                                                     optimizer,train_loader, val_loader,\n                                                      scheduler, epochs, \n                                                       device = device, cnn=cnn)\n        return train_losses, test_losses\n    else:\n        if torch.cuda.is_available():\n            trained_model = torch.load(load_model_weight)     \n        else:\n            trained_model = torch.load(load_model_weight, map_location=torch.device('cpu'))       \n        return trained_model","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:07.418793Z","iopub.execute_input":"2024-05-17T08:51:07.419206Z","iopub.status.idle":"2024-05-17T08:51:07.427758Z","shell.execute_reply.started":"2024-05-17T08:51:07.419175Z","shell.execute_reply":"2024-05-17T08:51:07.426592Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def load_and_retrain(load_model_weight, new_model_name, epochs, cnn=False):\n    trained_model = torch.load(load_model_weight) \n    #criterion = nn.MSELoss()\n    criterion = nn.HuberLoss()\n    optimizer = torch.optim.AdamW(trained_model.parameters(),lr=0.0001)\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, min_lr = 5e-7)\n    trained_model.train()\n    train_losses, test_losses=batch_gd_scheduler(trained_model,new_model_name, criterion, optimizer,\n                                                 train_loader, val_loader, \n                                                 scheduler, epochs, \n                                                 device = device, cnn=cnn)\n    return trained_model, train_losses, test_losses\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:08.658851Z","iopub.execute_input":"2024-05-17T08:51:08.659244Z","iopub.status.idle":"2024-05-17T08:51:08.667474Z","shell.execute_reply.started":"2024-05-17T08:51:08.659216Z","shell.execute_reply":"2024-05-17T08:51:08.666048Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def plot_losses(train_losses, test_losses):\n    # Plot the train loss and test loss per iteration\n    plt.plot(train_losses, label='train loss')\n    plt.plot(test_losses, label='val loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:09.707096Z","iopub.execute_input":"2024-05-17T08:51:09.707480Z","iopub.status.idle":"2024-05-17T08:51:09.714545Z","shell.execute_reply.started":"2024-05-17T08:51:09.707450Z","shell.execute_reply":"2024-05-17T08:51:09.713194Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_pred_n_acc(models, device, dataloader, num_iter):\n    i = 0\n    ypreds =[]\n    targets = []\n    #The last batch might not have size 128\n    while i< num_iter:\n        for data, target in dataloader:\n            #this empty list is to hold all models' preds\n            ypred = []    \n            data= data.to(device)\n            data = data.to(torch.float32)\n            target = target.to(torch.float32)\n            #append the 'i^th' target\n            targets.append(target)\n            for model in models:\n                model.eval()\n                yp = model(data)\n                yp = yp.detach().cpu().numpy()\n                ypred.append(yp)\n            #take the mean of all models' predictions\n            ypred = np.array(ypred).mean(axis = 0)\n            ypred = np.round(ypred)\n            i+=1\n            #append ypred, targets inside the 'i' loop\n            # append the 'i^th' mean prediction\n            ypreds.append(ypred)   \n            if i == num_iter:\n                break \n     #Do not convert ypreds, targets to np.array at this point,\n    #since the last batch has a different size, causing an error !\n    #CALCULATING ACCURACY    \n    yp =  np.concatenate([ypreds[j] for j in range(len(ypreds))], axis = 0)\n    tgs =  np.concatenate([targets[j] for j in range(len(targets))], axis = 0)\n    h11_acc = ((yp[:,0] == tgs[:,0]).sum())/len(yp)\n    h21_acc = ((yp[:,1] == tgs[:,1]).sum())/len(yp)\n    h31_acc = ((yp[:,2] == tgs[:,2]).sum())/len(yp)\n    h22_acc = ((yp[:,3] == tgs[:,3]).sum())/len(yp)\n    acc = [h11_acc*100,h21_acc*100,h31_acc*100,h22_acc*100 ]\n    return  yp, tgs, acc","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:11.223192Z","iopub.execute_input":"2024-05-17T08:51:11.223594Z","iopub.status.idle":"2024-05-17T08:51:11.237517Z","shell.execute_reply.started":"2024-05-17T08:51:11.223562Z","shell.execute_reply":"2024-05-17T08:51:11.236464Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"code","source":"load_model_weight = None\nepochs = 550\nnew_model_name = 'ResNet-LSTM-400'\ntrain_losses, test_losses=train_from_scratch_or_load(load_model_weight,new_model_name,  epochs, cnn = False)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T08:51:18.943879Z","iopub.execute_input":"2024-05-17T08:51:18.944345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, test_losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train/validation/test accuracy","metadata":{}},{"cell_type":"code","source":"#Train accuracy\nyt, tgt, acct = get_pred_n_acc([model], device, train_loader, num_iter = len(train_loader))\n#validation accuracy\nyv, tgv, accv = get_pred_n_acc([model], device, val_loader, num_iter = len(val_loader))\nprint(f'Train data, target shapes: {yt.shape, tgt.shape},\\n train accuracies {acct}')\nprint(f'Validation data, target shapes: {yv.shape, tgv.shape},\\n validation accuracies {accv}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inference: Test accuracy\nyts, tgts, accts = get_pred_n_acc([model], device, test_loader, num_iter = len(test_loader))\nprint(f'Test data, target shapes: {yts.shape, tgts.shape},\\n test accuracies {accts}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
